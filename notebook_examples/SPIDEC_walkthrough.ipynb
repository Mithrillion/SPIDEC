{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we provide a step-by-step walkthrough of the SPIDEC algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. General Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general procedure of SPIDEC consists of four steps. Given a time series X, we:\n",
    "\n",
    "    1. Apply a time-delay embedding to extract the subsequences (or general TD embedding vectors);\n",
    "    2. Find the transitive exclusive kNNs of each subsequence;\n",
    "    3. Find the UMAP embedding of the subsequences given the kNN graph;\n",
    "    4. Use HDBSCAN to find the subsequence clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extracting Subsequences / TD Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following function to extract TD embeddings from the time series data. When delta_t is 1, the extracted embeddings are simply sliding window subsequences. When skip_step is also 1, we extract maximally overlapping sliding windows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import typing\n",
    "\n",
    "def extract_td_embeddings(\n",
    "    arr: typing.Union[np.ndarray, torch.Tensor],\n",
    "    delta_t: int,\n",
    "    embedding_dim: int,\n",
    "    skip_step: int,\n",
    "    dim_order: str = \"dpt\",  # [(per-time) dim, (OG seq) position, (subseq) time]\n",
    ") -> torch.tensor:\n",
    "    source_tensor = torch.tensor(arr) if isinstance(arr, np.ndarray) else arr\n",
    "    td_embedding = F.unfold(\n",
    "        source_tensor.T.view(1, source_tensor.shape[1], 1, source_tensor.shape[0]),\n",
    "        (1, embedding_dim),\n",
    "        dilation=delta_t,\n",
    "        stride=skip_step,\n",
    "    ).view(source_tensor.shape[1], embedding_dim, -1)\n",
    "    if dim_order == \"dpt\":\n",
    "        td_embedding = td_embedding.permute(0, 2, 1)\n",
    "    elif dim_order == \"pdt\":\n",
    "        td_embedding = td_embedding.permute(2, 0, 1)\n",
    "    elif dim_order == \"dtp\":\n",
    "        pass\n",
    "    elif dim_order == \"ptd\":\n",
    "        td_embedding = td_embedding.permute(2, 1, 0)\n",
    "    elif dim_order == \"p_td\":\n",
    "        td_embedding = td_embedding.permute(2, 1, 0).flatten(-2, -1)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dim_order string!\")\n",
    "    return td_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Transitive Exclusion kNN with MPdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we need to go through several ideas:\n",
    "\n",
    "    1. Exclusion radius for subsequence nearest neighbours\n",
    "    2. Transitive exclusion\n",
    "    3. Applying transitive exclusion to MPdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exclusion Radius and Transitive Exclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of exclusion radius first appeared in subsequence 1-nearest neighbour setting (such as in the Matrix Profile algorithm), when we want to avoid \"trivial matches\" - a subsequence $A$ matching with another subsequence $B$ that almost temporally overlaps with $A$. It is not helpful if an algorithm simply tells us that the subsequence at time $t$ is similar to the subsequence at $t+1$. Therefore, we often prohibit an algorithm from reporting subsequences within temporal separation $r$ as a valid nearest neighbour. Naturally, this idea extends to the k-nearest-neighbour case as well, when we want nearest neighbours of a subsequence to be outside its temporal exclusion raidus. However, in the kNN setting, there is the added problem of two nearest neighbours of $A$, let us say $B$ and $C$, being very close temporally to each other (i.e. are trivial matches of each other). This is problematic because if we want to know how many times the time series returns to a similar state as $A$, then $B$ and $C$ most likely describe the same instance of a \"return\", but are counted twice. Moreover, for some subsequences, all their kNNs may describe unique recurrences, but for others, it is possible most of their kNNs are duplicated counts due to such \"transitive\" trivial matches, leading to inconsistencies between the meaning of kNNs for different subsequences. It is therefore important to follow a procedure which produces kNNs of subsequences without any transitive trivial matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A General Function for Exclusion 1NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a general function for computing Euclidean distance nearest neighbours with exclusion radius, using PyKeOps for hardware acceleration. We use $X_i - X_j$ to symbolically encode the pairwise subsequence distances, and $I_i - I_j$ to encode pairwise time index differences, then set subsequence distance to infinity if within exclusion radius. This is not as optimised as some subsequence-specific 1NN algorithms such as Matrix Profile, but can be generalised to non-contiguous sliding windows, such as TD embedding with delays, or feature maps of TD vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pykeops.torch as ktorch\n",
    "\n",
    "def exclusion_knn_search(\n",
    "    X: torch.Tensor, k: int, excl: int\n",
    ") -> typing.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    X_i = ktorch.LazyTensor(X[:, None, :])\n",
    "    X_j = ktorch.LazyTensor(X[None, :, :])\n",
    "    indices = torch.arange(len(X), device=X.device).float()\n",
    "    I_i = ktorch.LazyTensor(indices[:, None, None])\n",
    "    I_j = ktorch.LazyTensor(indices[None, :, None])\n",
    "    Diag_ij = float(excl) - (I_i - I_j).abs()\n",
    "    D_ij = ((X_i - X_j) ** 2).sum(-1)\n",
    "    D_ij = Diag_ij.ifelse(np.inf, D_ij)\n",
    "    D, I = D_ij.Kmin_argKmin(K=k, dim=1)\n",
    "    D = torch.sqrt(D)\n",
    "    return D, I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### General Transitive Exclusion kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it is easy to apply exclusion radius on the input subsequence, it is much harder to do so with kNN entries, as the exact temporal regions to be excluded depends on what kNN entries are returned, and the order we apply exclusion radii changes what kNN entries are considered valid. We want to achieve the following:\n",
    "\n",
    "    * When multiple pre-exclusion kNNs are trivial matches of each other, the closest to the input subsequence should be returned as a nearest neighbour\n",
    "    * We want to obtain the same number of nearest neighbours (k) for each subsequence\n",
    "\n",
    "It is possible to first find a large number of nearest neighbours for each subsequence, then prune them to $k$ transitive-exclusive NNs in postprocessing, but a more straightforward way is to run exclusion 1NN $k$ times, expanding the exclusion regions after each iteration.\n",
    "\n",
    "In the code section below, we use a variable I_prev to keep track of all centres of exclusion zones, which we initialise to the position of input subsequences themselves at the start, then run a transitive_exclusion_step, which will be the same as exclusion 1NN at the start. After each iteration, we append the positions of the most recently-found nearest neighbour for each subsequence to I_prev, so in the next transitive_exclusion_step, we will also exclude a radius-r zone around the nearest neigbhours from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_search(feats: torch.Tensor, k: int) -> typing.Tuple[torch.tensor, torch.tensor]:\n",
    "    X_i = ktorch.LazyTensor(feats[:, None, :])\n",
    "    X_j = ktorch.LazyTensor(feats[None, :, :])\n",
    "    D_ij = ((X_i - X_j) ** 2).sum(-1)\n",
    "    D, I = D_ij.Kmin_argKmin(K=k, dim=1)\n",
    "    D = torch.sqrt(D)\n",
    "    return D, I\n",
    "\n",
    "\n",
    "def transitive_exclusion_step(\n",
    "    feats: torch.Tensor, excl: int, I_prev: torch.Tensor = None\n",
    ") -> typing.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    X_i = ktorch.LazyTensor(feats[:, None, :])\n",
    "    X_j = ktorch.LazyTensor(feats[None, :, :])\n",
    "    indices = torch.arange(len(feats), device=feats.device).float()\n",
    "    if I_prev is None:\n",
    "        I_prev = indices[:, None]\n",
    "    I_i = ktorch.LazyTensor(I_prev[:, None, :].float())\n",
    "    I_j = ktorch.LazyTensor(indices[None, :, None])\n",
    "    Diag_ij = (float(excl) - (I_i - I_j).abs()).ifelse(1, 0).sum(-1) - 0.5\n",
    "    D_ij = ((X_i - X_j) ** 2).sum(-1)\n",
    "    D_ij = Diag_ij.ifelse(np.inf, D_ij)\n",
    "    D, I = D_ij.min_argmin(dim=1)\n",
    "    D = torch.sqrt(D)\n",
    "    return D, torch.cat((I_prev, I), dim=-1)\n",
    "\n",
    "\n",
    "def transitive_exclusion_knn_search(\n",
    "    feats: torch.Tensor,\n",
    "    k: int,\n",
    "    excl: int,\n",
    ") -> typing.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    if excl == 0:\n",
    "        return knn_search(feats, k)\n",
    "    D_list = []\n",
    "    for i in range(k):\n",
    "        Dk, Ik = transitive_exclusion_step(\n",
    "            feats, excl, I_prev=Ik if i != 0 else None\n",
    "        )\n",
    "        D_list += [Dk]\n",
    "    return torch.cat(D_list, dim=-1), Ik[:, 1:].long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why Transitive Exclusion is Important for Subsequence Density Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we consider a \"frequent pattern\" or \"recurrent state\" for a time series, what we really want to know is how often and how precisely the time series returns to the same state. Without transitive exclusion, we cannot accurately estimate that from a kNN graph. Imagine a racecar moving through a track (or any process going through a phase space, generating a time series), and we record the position of the car at equal time intervals. The car will regularly return to each corners on the track, but it will pass through some corners faster and some slower. Effectively, we will have higher sample rate at slower corners, and the position states at slower corners will have more trivial (coming from the same lap) matches. Therefore, without transitive exclusion, it would appear that the data is denser around slower corners, because each actual revisit will results in several mutually trivial nearest neighbour matches, whereas around faster corners, it is possible that only one position state from each revisit gets matched as a nearest neighbour. However, this is not reflecting the truth, as the car revisits every corner the same amount of times, and the frequency and exactness at which the car revisits a state should only be dependent on how closely the position states match at the most matching positions from each lap. To do so, we would have to introduce transitive exclusivity in the kNN search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transitive Exclusion kNN for MPdist / Minimal Bag-of-subsequences Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code section condenses the above steps into a single function for MPdist, which is the minimal (or qth quantile) distance between all subsequences of two time series. Here, we wish to find the MPdist kNNs for all subsequences in a longer time series, therefore we will need to further break down subsequences into subsequences of subsequences, which will be used to find the MPdist values for the \"meta\" subsequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpdist_exclusion_knn_search(\n",
    "    feats: torch.Tensor,\n",
    "    k: int,\n",
    "    bag_size: int,\n",
    "    tau: int = 1,\n",
    "    skip_step: int = 1,\n",
    "    quantile: float = 0,\n",
    "):\n",
    "    pth_smallest = int(bag_size * quantile)\n",
    "    # first compute subseq knn dists\n",
    "    D_s, I_s = transitive_exclusion_knn_search(feats, k, bag_size)\n",
    "    # break knn dist array into sliding windows\n",
    "    D_s_sets, I_s_sets = extract_td_embeddings(\n",
    "        D_s, tau, bag_size, skip_step, \"ptd\"\n",
    "    ), extract_td_embeddings(I_s.float(), tau, bag_size, skip_step, \"ptd\")\n",
    "    # now the format is (metaseq position, subseq position, knn list)\n",
    "    # now pre-allocate mpdist and index arrays\n",
    "    D_m, I_m = (\n",
    "        torch.zeros((len(D_s_sets), k)).float().to(feats.device),\n",
    "        torch.zeros((len(D_s_sets), k)).int().to(feats.device)\n",
    "        - 2\n",
    "        * bag_size,  # ensure no match on initial run (can be arbitrary \"impossible\" value)\n",
    "    )\n",
    "    mask = torch.zeros_like(I_s_sets).bool().to(feats.device)\n",
    "    for i in range(k):\n",
    "        diff = I_m[:, None, None, :] - I_s_sets[:, :, :, None]\n",
    "        mask = mask | torch.any(\n",
    "            torch.abs(diff) < bag_size // skip_step,\n",
    "            -1,\n",
    "        )\n",
    "\n",
    "        masked_D_s_sets = torch.where(\n",
    "            ~mask, D_s_sets, torch.inf\n",
    "        )  # (metaseq, subseq, knn)\n",
    "        match_choice_set, knn_indices = torch.min(\n",
    "            masked_D_s_sets, dim=2\n",
    "        )  # (metaseq, subseq)\n",
    "        # knn_indices records which knn is the current unmasked smallest per metaseq per subseq\n",
    "\n",
    "        if quantile == 0:\n",
    "            D_m[:, i], min_subseq_pos = torch.min(match_choice_set, dim=1)\n",
    "            # min_subseq_pos records which subseq produces the smallest dist per metaseq\n",
    "        else:\n",
    "            top_d, top_i = torch.topk(\n",
    "                match_choice_set, pth_smallest, dim=1, largest=False\n",
    "            )  # (metaseq)\n",
    "            D_m[:, i], min_subseq_pos = top_d[..., -1], top_i[..., -1]\n",
    "\n",
    "        # we still need the kth NN indices\n",
    "        min_subseq_knn_indices = knn_indices.gather(\n",
    "            1, min_subseq_pos[:, None]\n",
    "        )  # (metaseq)\n",
    "        I_m[:, i] = (\n",
    "            I_s_sets.gather(1, min_subseq_pos[:, None, None].expand(-1, -1, k))\n",
    "            .squeeze()\n",
    "            .gather(1, min_subseq_knn_indices)\n",
    "            .squeeze()\n",
    "        )  # (metaseq)\n",
    "\n",
    "    return D_m, I_m.long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Local Density and UMAP Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Clustering with HDBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tda_cuml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
